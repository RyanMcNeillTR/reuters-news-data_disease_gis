---
title: "R Notebook"
output: html_notebook
---

This notebook tests some code to iterate over the area of polygons where spillover occurrence location is approximate, as it is in most of our data to one degree or another.

This is modelled on Pigott et al's Ebola risk map.

This test file uses a synthetic data set where all but one data point is a point, and a single occurrence has a large polygon associated with it.

For speed, this file uses the 'hits' and 'controls' dataFRAMES from analysis_1.Rmd as a starting point.

rough outline:
Load shp with shapefile(s)
Flag occurrence data as point or polygon

```{r, echo = FALSE}

library(seegSDM)

library(tidymodels)
library(tidyverse)
library(data.table)

library(baguette)

library(sf)
library(rgeos)

```

Remove full covariate data set loaded in analysis_1.Rmd, if it is stil present.

```{r}

rm(long_data_joined_read)

```

Read in data from Ryan, convert points object into coordinates and flag occurrences that are coded to polygons based on whether their point is a centroid or not. 

```{r}

raw_occ <- readRDS("C:/Users/0146156/Documents/mb_checks/all_combined.rds")

coords <- raw_occ$point_geom %>% st_coordinates()

hits <- raw_occ %>% 
  mutate(poly = ifelse(str_detect(precision, "[Cc]entroid"), TRUE, FALSE)) %>%
  cbind(coords)

hits %>% st_drop_geometry() %>% group_by(poly) %>% count
hits %>% st_drop_geometry() %>% filter(is.na(poly))

# separate hits into poly and point data frames 
dat_poly <- hits %>% filter(poly)
dat_pt <- hits %>% filter(!poly)

```

Load 5km raster template.

```{r}

library(raster)

template <- raster("C:/Users/0146156/Documents/mb_checks/hansen_datamask.tif")

```

Generate artificial absence records and a fictional covariate.

```{r}
library(dismo)

set.seed(123)

tmp <- randomPoints(template, 1000)

bg <- tmp %>% as_tibble() %>% 
  mutate(spillover = FALSE) %>%
  rowid_to_column("rowid") %>%
  mutate(fic_cov = runif(nrow(bg), 1, 10)) %>%
  select(rowid, spillover, fic_cov, x, y)

```

Use lapply to loop over each row in shp, our dataframe of polygons, and create lists of coordinates associated with the polygons.

```{r}

shp <- st_sf(dat_poly$poly_geom)

pt_list <- lapply(1:nrow(shp),

  function(i, shp, template) {
    poly <- shp[i, ] #  grab one polygon
    
    # buffer to cover the center of at least one pixel - we may not need this
    #d <- sqrt(2 * 2500 ^ 2) + 1
    #d <- d * 10 ^ -5 # rough conversion to decimal degrees
    #poly <- gBuffer(poly, width = d)
    
    # rasterize the polygon and get coordinates
    tmp <- rasterize(poly, template)
    pts <- xyFromCell(tmp, which(!is.na(getValues(tmp))))
    
    return (pts)
    
  },

  shp, template)

```
Loop through our polygon shapefile shp and create a new dataframe where each coordinate has its own row and other variables are duplicated, for sampling purposes later.

*** This will work once dat_new has lat/long values as well (and migh tneed to fix names etc)

```{r}

# starting a new data frame that will eventually have points and polygons 
# weights are always 1 for point occurrences 
dat_new <- dat_pt %>% 
  select(-point_geom) %>%
  st_drop_geometry() %>% 
  mutate(wt = 1)


# loop through polygon dataframe again
for(i in 1:nrow(shp)) {
  
  # pull the rowid, points associated with this occurrence and point count 
  id <- shp$id[i]
  
  pts <- pt_list[[i]]
  
  n <- nrow(pts)
  
  # return a warning if no points were found 
  #if (n == 0) {
  #  warning(paste("Polygon for occurrence ", rowid, "returned zero coordinates!" )
  #  break
  #}
  
  # pull other data and repeat it to match each point 
  info <- dat_poly[i, ]
  info_repeat <- info[rep(1:nrow(info), each = n), ]
  
  # add weights
  info_repeat$wt <- 1/n
  
  # add coordinates
  final <- cbind(info_repeat, pts)
  
  dat_new <- rbind(dat_new, final)
  
}

```


